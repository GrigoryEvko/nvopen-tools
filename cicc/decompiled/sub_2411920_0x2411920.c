// Function: sub_2411920
// Address: 0x2411920
//
__int64 __fastcall sub_2411920(__int64 a1, __int64 **a2)
{
  __int64 *v4; // r14
  __int64 v5; // rax
  unsigned __int64 v6; // rax
  __int64 v7; // rdi
  __int64 v8; // rcx
  __int64 v9; // rax
  __int64 v10; // rdx
  __int64 v11; // rax
  unsigned __int64 v12; // rax
  __int64 v13; // rdi
  __int64 v14; // rcx
  __int64 v15; // rax
  __int64 v16; // rdi
  __int64 v17; // rcx
  __int64 v18; // rdx
  __int64 v19; // rax
  __int64 v20; // rdi
  __int64 v21; // rcx
  __int64 v22; // rdx
  __int64 v23; // rax
  __int64 v24; // rdx
  unsigned __int64 v25; // rax
  __int64 *v26; // rsi
  unsigned __int64 v27; // rax
  __int64 v28; // rdi
  __int64 v29; // rcx
  __int64 v30; // rax
  __int64 v31; // rdi
  __int64 v32; // rcx
  __int64 v33; // rdx
  __int64 v34; // rax
  __int64 v35; // rdi
  __int64 v36; // rcx
  __int64 v37; // rdx
  __int64 v38; // rax
  __int64 v39; // rdx
  unsigned __int64 v40; // rax
  __int64 *v41; // rsi
  unsigned __int64 v42; // rax
  __int64 v43; // rdi
  __int64 v44; // rcx
  __int64 v45; // rax
  __int64 v46; // rdx
  unsigned __int64 v47; // rax
  __int64 *v48; // rsi
  unsigned __int64 v49; // rax
  __int64 *v50; // rsi
  unsigned __int64 v51; // rax
  __int64 v52; // rdi
  __int64 v53; // rcx
  __int64 v54; // rax
  __int64 v55; // rdi
  __int64 v56; // rcx
  __int64 v57; // rdx
  __int64 v58; // rax
  __int64 v59; // rdi
  __int64 v60; // rcx
  __int64 v61; // rdx
  __int64 v62; // rax
  __int64 v63; // rdi
  __int64 v64; // rcx
  __int64 v65; // rdx
  __int64 v66; // rax
  __int64 v67; // rdx
  unsigned __int64 v68; // rax
  __int64 *v69; // rsi
  unsigned __int64 v70; // rax
  __int64 v71; // rcx
  __int64 v72; // rdi
  __int64 v73; // rax
  unsigned __int8 *v74; // rdi
  __int64 v75; // rdx
  __int64 *v76; // rax
  __int64 v77; // rcx
  __int64 v78; // r8
  __int64 v79; // r9
  __int64 *v80; // rax
  __int64 v81; // rcx
  __int64 v82; // r8
  __int64 v83; // r9
  __int64 *v84; // rax
  __int64 v85; // rcx
  __int64 v86; // r8
  __int64 v87; // r9
  __int64 *v88; // rax
  __int64 v89; // rcx
  __int64 v90; // r8
  __int64 v91; // r9
  __int64 *v92; // rax
  __int64 v93; // rcx
  __int64 v94; // r8
  __int64 v95; // r9
  __int64 *v96; // rax
  __int64 v97; // rcx
  __int64 v98; // r8
  __int64 v99; // r9
  __int64 *v100; // rax
  __int64 v101; // rcx
  __int64 v102; // r8
  __int64 v103; // r9
  __int64 *v104; // rax
  __int64 v105; // rcx
  __int64 v106; // r8
  __int64 v107; // r9
  __int64 *v108; // rax
  __int64 v109; // rcx
  __int64 v110; // r8
  __int64 v111; // r9
  __int64 *v112; // rax
  __int64 v113; // rcx
  __int64 v114; // r8
  __int64 v115; // r9
  __int64 *v116; // rax
  __int64 v117; // rcx
  __int64 v118; // r8
  __int64 v119; // r9
  __int64 *v120; // rax
  __int64 v121; // rcx
  __int64 v122; // r8
  __int64 v123; // r9
  __int64 *v124; // rax
  __int64 v125; // rcx
  __int64 v126; // r8
  __int64 v127; // r9
  __int64 *v128; // rax
  __int64 v129; // rcx
  __int64 v130; // r8
  __int64 v131; // r9
  __int64 *v132; // rax
  __int64 v133; // rcx
  __int64 v134; // r8
  __int64 v135; // r9
  __int64 *v136; // rax
  __int64 v137; // rcx
  __int64 v138; // r8
  __int64 v139; // r9
  __int64 *v140; // rax
  __int64 v141; // rcx
  __int64 v142; // r8
  __int64 v143; // r9
  __int64 *v144; // rax
  __int64 v145; // rcx
  __int64 v146; // r8
  __int64 v147; // r9
  __int64 *v148; // rax
  __int64 v149; // rcx
  __int64 v150; // r8
  __int64 v151; // r9
  __int64 *v152; // rax
  __int64 v153; // rcx
  __int64 v154; // r8
  __int64 v155; // r9
  __int64 *v156; // rax
  __int64 v157; // rcx
  __int64 v158; // r8
  __int64 v159; // r9
  __int64 v161[10]; // [rsp+0h] [rbp-50h] BYREF

  v4 = *a2;
  v161[0] = 0;
  v161[0] = sub_A7A090(v161, v4, -1, 41);
  v5 = sub_A77AB0(v4, 0x55u);
  v161[0] = sub_A7B440(v161, v4, -1, v5);
  v6 = sub_A7A090(v161, v4, 0, 79);
  v7 = *(_QWORD *)a1;
  v8 = *(_QWORD *)(a1 + 120);
  v161[0] = v6;
  v9 = sub_BA8C10(v7, (__int64)"__dfsan_union_load", 0x12u, v8, v6);
  *(_QWORD *)(a1 + 288) = v10;
  *(_QWORD *)(a1 + 280) = v9;
  v161[0] = 0;
  v161[0] = sub_A7A090(v161, v4, -1, 41);
  v11 = sub_A77AB0(v4, 0x55u);
  v161[0] = sub_A7B440(v161, v4, -1, v11);
  v12 = sub_A7A090(v161, v4, 0, 79);
  v13 = *(_QWORD *)a1;
  v14 = *(_QWORD *)(a1 + 128);
  v161[0] = v12;
  v15 = sub_BA8C10(v13, (__int64)"__dfsan_load_label_and_origin", 0x1Du, v14, v12);
  v16 = *(_QWORD *)a1;
  v17 = *(_QWORD *)(a1 + 136);
  *(_QWORD *)(a1 + 304) = v18;
  *(_QWORD *)(a1 + 296) = v15;
  v19 = sub_BA8CA0(v16, (__int64)"__dfsan_unimplemented", 0x15u, v17);
  v20 = *(_QWORD *)a1;
  v21 = *(_QWORD *)(a1 + 144);
  *(_QWORD *)(a1 + 320) = v22;
  *(_QWORD *)(a1 + 312) = v19;
  v23 = sub_BA8CA0(v20, (__int64)"__dfsan_wrapper_extern_weak_null", 0x20u, v21);
  v161[0] = 0;
  *(_QWORD *)(a1 + 336) = v24;
  *(_QWORD *)(a1 + 328) = v23;
  v25 = sub_A7A090(v161, *a2, 1, 79);
  v26 = *a2;
  v161[0] = v25;
  v27 = sub_A7A090(v161, v26, 2, 79);
  v28 = *(_QWORD *)a1;
  v29 = *(_QWORD *)(a1 + 152);
  v161[0] = v27;
  v30 = sub_BA8C10(v28, (__int64)"__dfsan_set_label", 0x11u, v29, v27);
  v31 = *(_QWORD *)a1;
  v32 = *(_QWORD *)(a1 + 160);
  *(_QWORD *)(a1 + 352) = v33;
  *(_QWORD *)(a1 + 344) = v30;
  v34 = sub_BA8CA0(v31, (__int64)"__dfsan_nonzero_label", 0x15u, v32);
  v35 = *(_QWORD *)a1;
  v36 = *(_QWORD *)(a1 + 168);
  *(_QWORD *)(a1 + 368) = v37;
  *(_QWORD *)(a1 + 360) = v34;
  v38 = sub_BA8CA0(v35, (__int64)"__dfsan_vararg_wrapper", 0x16u, v36);
  v161[0] = 0;
  *(_QWORD *)(a1 + 384) = v39;
  *(_QWORD *)(a1 + 376) = v38;
  v40 = sub_A7A090(v161, *a2, 1, 79);
  v41 = *a2;
  v161[0] = v40;
  v42 = sub_A7A090(v161, v41, 0, 79);
  v43 = *(_QWORD *)a1;
  v44 = *(_QWORD *)(a1 + 232);
  v161[0] = v42;
  v45 = sub_BA8C10(v43, (__int64)"__dfsan_chain_origin", 0x14u, v44, v42);
  v161[0] = 0;
  *(_QWORD *)(a1 + 528) = v46;
  *(_QWORD *)(a1 + 520) = v45;
  v47 = sub_A7A090(v161, *a2, 1, 79);
  v48 = *a2;
  v161[0] = v47;
  v49 = sub_A7A090(v161, v48, 2, 79);
  v50 = *a2;
  v161[0] = v49;
  v51 = sub_A7A090(v161, v50, 0, 79);
  v52 = *(_QWORD *)a1;
  v53 = *(_QWORD *)(a1 + 240);
  v161[0] = v51;
  v54 = sub_BA8C10(v52, (__int64)"__dfsan_chain_origin_if_tainted", 0x1Fu, v53, v51);
  v55 = *(_QWORD *)a1;
  v56 = *(_QWORD *)(a1 + 248);
  *(_QWORD *)(a1 + 544) = v57;
  *(_QWORD *)(a1 + 536) = v54;
  v58 = sub_BA8CA0(v55, (__int64)"__dfsan_mem_origin_transfer", 0x1Bu, v56);
  v59 = *(_QWORD *)a1;
  v60 = *(_QWORD *)(a1 + 256);
  *(_QWORD *)(a1 + 560) = v61;
  *(_QWORD *)(a1 + 552) = v58;
  v62 = sub_BA8CA0(v59, (__int64)"__dfsan_mem_shadow_origin_transfer", 0x22u, v60);
  v63 = *(_QWORD *)a1;
  v64 = *(_QWORD *)(a1 + 264);
  *(_QWORD *)(a1 + 576) = v65;
  *(_QWORD *)(a1 + 568) = v62;
  v66 = sub_BA8CA0(v63, (__int64)"__dfsan_mem_shadow_origin_conditional_exchange", 0x2Eu, v64);
  v161[0] = 0;
  *(_QWORD *)(a1 + 592) = v67;
  *(_QWORD *)(a1 + 584) = v66;
  v68 = sub_A7A090(v161, *a2, 1, 79);
  v69 = *a2;
  v161[0] = v68;
  v70 = sub_A7A090(v161, v69, 4, 79);
  v71 = *(_QWORD *)(a1 + 272);
  v72 = *(_QWORD *)a1;
  v161[0] = v70;
  v73 = sub_BA8C10(v72, (__int64)"__dfsan_maybe_store_origin", 0x1Au, v71, v70);
  v74 = *(unsigned __int8 **)(a1 + 288);
  *(_QWORD *)(a1 + 608) = v75;
  *(_QWORD *)(a1 + 600) = v73;
  v76 = (__int64 *)sub_BD3990(v74, (__int64)"__dfsan_maybe_store_origin");
  sub_2411830((__int64)v161, a1 + 616, v76, v77, v78, v79);
  v80 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 304), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v80, v81, v82, v83);
  v84 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 320), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v84, v85, v86, v87);
  v88 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 336), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v88, v89, v90, v91);
  v92 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 352), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v92, v93, v94, v95);
  v96 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 368), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v96, v97, v98, v99);
  v100 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 384), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v100, v101, v102, v103);
  v104 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 400), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v104, v105, v106, v107);
  v108 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 416), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v108, v109, v110, v111);
  v112 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 432), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v112, v113, v114, v115);
  v116 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 448), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v116, v117, v118, v119);
  v120 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 464), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v120, v121, v122, v123);
  v124 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 480), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v124, v125, v126, v127);
  v128 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 496), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v128, v129, v130, v131);
  v132 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 512), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v132, v133, v134, v135);
  v136 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 528), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v136, v137, v138, v139);
  v140 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 544), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v140, v141, v142, v143);
  v144 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 560), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v144, v145, v146, v147);
  v148 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 576), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v148, v149, v150, v151);
  v152 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 592), a1 + 616);
  sub_2411830((__int64)v161, a1 + 616, v152, v153, v154, v155);
  v156 = (__int64 *)sub_BD3990(*(unsigned __int8 **)(a1 + 608), a1 + 616);
  return sub_2411830((__int64)v161, a1 + 616, v156, v157, v158, v159);
}
